---
title: "IE48B_HW3_Report"
author: "Yusuf Sina Öztürk"
date: "12/12/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, libraries, include= FALSE}
library(data.table)
library(ggplot2)
library(TSdist)
library(dtw)
library(rpart)
library(rattle)
library(zoo)
library(repr)
library(TSrepr)
library(Rcpp)
library(TunePareto)
```

### Introduction

In this report, I am going to compare different distance measures with using different representations settings in training data. I will do my classification with using Neighbourhood Search (NN) with using Cross Validation. I will run this code for 5 different univariate time series data set. I took all my datasets from [Time Sereies Classification]( www.timeseriesclassification.com) web-site which provides a lot univariate time series dataset.

I choose my datasets with respect to their volume. Because in distance measure competition phase, some approaches takes some time to compute. Therefore, I try to choose my datasets with around 100 timeseries for both train and data sets and with around 150 variable in it.

Here is name of the choosen data sets:

- ECG200
- PowerCons
- Plane
- GunPointAgeSpan
- GunPointMaleVersusFemale

I also need to choose 2 different representation approach to follow, which are:

- Raw Data (without any implementation)
- Piecewise Aggregate Approximation
- Symbolic Aggregate Approximation (SAX)

Because PAA and SAX using some parameters in it, I will be use two different parameter set for representing my data.

Parameter set for PAA is:
- Segment Length: 5
- Segment Length: 10

Parameter set for SAX is:
- Segment length: 4, alphabet size: 5
- Segment length: 8, alphabet size: 4

Moreover, I will going to use 4 different distance measure:

- Euclidean Distance
- Dynamic Time Warping (DTW)
- LCSS
- ERP

Also these distance measurements uses some parameters (euclidean distance), however, I am going to use default parameters for calculating distances.

I also used Cross Validation approach to have more valid results and it's parameters is:

- k = {1,3,5}
- n-fold = 10
- repeats = 5


After introducing all the approaches and parameters i will going to use, we can start with manipulating the data. 

### Data Manipulations and Visualitions

```{r, Path Determination, warning=F,fig.width=10}
# Path Determination

# assuming you have the data folder in your working directory in the following format:
# 'working_directory/ClassificationData/dataset_name/'
current_folder=getwd()
dataset='ECG200'
main_path=sprintf('%s/ClassificationData/%s',current_folder,dataset)
dist_path=sprintf('%s/ClassificationData/%s/distances/%s',current_folder,dataset,dataset)
```

I wrote this code like this, when I try to analyse different dataset, I only need to change `dateset` name and run the code from top to the bottom so I don't waste time to change all the parameters and variable names. 

Now, we can continue with reading data from local.

```{r, Reading the univariate data from local, warning=F,fig.width=10}
# Reading the univariate data from local

train_data_path=sprintf('%s/%s_TRAIN.txt',main_path,dataset)
traindata=as.matrix(fread(train_data_path))

test_data_path=sprintf('%s/%s_TEST.txt',main_path,dataset)
testdata=as.matrix(fread(test_data_path))

```

Because datasets has the class information in its first row, first thing to do is take that information in another object.

```{r, First column is the class data of time series, warning=F,fig.width=10}

# First column is the class data of time series
trainclass <- traindata[,1]
testclass <- testdata[,1]
allclass <- c(trainclass, testclass)

```

Now, in order to transform data to long format, I added needed variables into dataset.

```{r, create long format of the data, warning=F,fig.width=10}

#create long format of the data
traindata <- as.data.table(traindata)
setnames(traindata,'V1','class')
traindata = traindata[order(class)]
traindata[,class:=as.character(class)]
traindata[,id:=1:.N]
head(traindata)

```

In order to work easily and visualize our data with ggplot, I melted the data to long format.

```{r, melt the data for long format, warning=F,fig.width=10}

#melt the data for long format
long_train = melt(traindata,id.vars=c('id','class'))
long_train[,time:=as.numeric(gsub("\\D", "", variable))-1]
long_train=long_train[,list(id,class,time,value)]
long_train=long_train[order(id,time)]
head(long_train)

```
Now, we can check, how the time series look like with using `ggplot` with respect to its class information.

```{r, Visualize the data based on Class, warning=F,fig.width=10}

# Visualize the data based on Class
ggplot(long_train, aes(time,value)) + geom_line(aes(color=as.character(id))) +
  facet_wrap(~class)

```

So as we can see from the plot, classes seems not too much different from each other for `ECG200` dataset. But we will see how it goes with our approaches.

Before going to represent our data, I need to get some information from original time series.

```{r, Instance Characteristics, warning=F,fig.width=10}

# Sort long table in order to make sure for last time
long_train = long_train[order(id,time)]

# Instance Characteristics
tlength=ncol(traindata) - 2
n_series_train=nrow(traindata)
n_series_test=nrow(testdata)

```

### Piecewise Aggregate Approximations

#### Parameter Set 1 (segment length = 5)

Defining needed variables:

```{r, Set parameters for PAA1, warning=F,fig.width=10}
#Parameter Set 1

selected_series=1
segment_length=5
paa_rep = vector("numeric",)
control = 0
paa_rep_all = vector("numeric",)
loop_indise = n_series_train * ceiling(tlength/5)

```

Now, we can represent our time series with using `repr_paa` function for each time series in train data.

```{r, PAA1, warning=F,fig.width=10}

for( i in 1:n_series_train){
  
  selected_series = i
  temp_data_ts = long_train[id == selected_series]$value
  temp_paa_rep=repr_paa(temp_data_ts, segment_length, meanC)
  paa_rep = append(paa_rep, temp_paa_rep)
  
}

```

Because we lose some data point, we need to fill those data points. However, each dataset has different number of variables, I need to write the code for each dataset.

```{r, PAA1 2, warning=F,fig.width=10}

for( i in 1:loop_indise){
  if(dataset == "ECG200"){
    if(control != 19){
      temp = rep(paa_rep[i], times = 5)
      paa_rep_all = append(paa_rep_all,temp)
      control = control + 1
    }
    else{
      paa_rep_all = append(paa_rep_all,paa_rep[i])
      control = 0
    }
  }
  if(dataset == "PowerCons"){
    if(control != 28){
      temp = rep(paa_rep[i], times = 5)
      paa_rep_all = append(paa_rep_all,temp)
      control = control + 1
    }
    else{
      temp = rep(paa_rep[i], times = 4)
      paa_rep_all = append(paa_rep_all,temp)
      control = 0
    }
  }
  if(dataset == "Plane"){
    if(control != 28){
      temp = rep(paa_rep[i], times = 5)
      paa_rep_all = append(paa_rep_all,temp)
      control = control + 1
    }
    else{
      temp = rep(paa_rep[i], times = 4)
      paa_rep_all = append(paa_rep_all,temp)
      control = 0
    }
  }
  if(dataset == "GunPointMaleVersusFemale"){
    temp = rep(paa_rep[i], times = 5)
    paa_rep_all = append(paa_rep_all,temp)
  }
  if(dataset == "GunPointAgeSpan"){
    temp = rep(paa_rep[i], times = 5)
    paa_rep_all = append(paa_rep_all,temp)
  }
}

long_train[,paa_rep := paa_rep_all]
head(long_train)

```

So we have a new representation column in our `long_train`.

#### Parameter Set 2 (segment length = 10)

Defining needed variables:

```{r, Set parameters for PAA2, warning=F,fig.width=10}

segment_length=10
paa_rep_2 = vector("numeric",)
control = 0
paa_rep_all_2 = vector("numeric",)
loop_indise = n_series_train * ceiling(tlength/10)

```

Create new representation with `repr_paa``

```{r, PAA2, warning=F,fig.width=10}

for( i in 1:n_series_train){
  
  selected_series = i
  temp_data_ts = long_train[id == selected_series]$value
  temp_paa_rep=repr_paa(temp_data_ts, segment_length, meanC)
  paa_rep_2 = append(paa_rep_2, temp_paa_rep)
  
}

```

Also the same issue for parameter set 1, for each dataset I need to add one if clause.

```{r, PAA2 2, warning=F,fig.width=10}

for( i in 1:loop_indise){
  if(dataset == "ECG200"){
    if(control != 9){
      temp = rep(paa_rep_2[i], times = 10)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = control + 1
    }
    else{
      temp = rep(paa_rep_2[i], times = 6)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = 0
    }
  }
  if(dataset == "PowerCons"){
    if(control != 14){
      temp = rep(paa_rep_2[i], times = 10)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = control + 1
    }
    else{
      temp = rep(paa_rep_2[i], times = 4)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = 0
    }
  }
  if(dataset == "Plane"){
    if(control != 14){
      temp = rep(paa_rep_2[i], times = 10)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = control + 1
    }
    else{
      temp = rep(paa_rep_2[i], times = 4)
      paa_rep_all_2 = append(paa_rep_all_2,temp)
      control = 0
    }
  }
  if(dataset == "GunPointMaleVersusFemale"){
    temp = rep(paa_rep_2[i], times = 10)
    paa_rep_all_2 = append(paa_rep_all_2,temp)
  }
  if(dataset == "GunPointAgeSpan"){
    temp = rep(paa_rep_2[i], times = 10)
    paa_rep_all_2 = append(paa_rep_all_2,temp)
  }
}

long_train[,paa_rep_2 := paa_rep_all_2]
head(long_train)

```

Here, we have two new represantation for training data. We also create additional representation with using SAX approach.

### Symbolic Aggregate Approximation

#### Parameter Set 1 (segment length = 4, alphabet size = 5)

Defining needed variables:

```{r, Set parameters for SAX1, warning=F,fig.width=10}

sax_segment_length=4
sax_alphabet_size=5
sax_rep = vector("character",)
sax_rep_all = vector("character",)
loop_indise = n_series_train * ceiling(tlength/4)

```

Now, we can represent our time series representation with using `repr_sax` function for each time series in train data.

```{r, SAX1, warning=F,fig.width=10}

for( i in 1:n_series_train){
  
  selected_series = i
  temp_data_ts = long_train[id == selected_series]$value
  temp_sax_rep=repr_sax(temp_data_ts, q = sax_segment_length, a = sax_alphabet_size)
  sax_rep = append(sax_rep, temp_sax_rep)
  
}

```

Here, we also needed to write if clause for each data set.

```{r, SAX1 2, warning=F,fig.width=10}

for( i in 1:loop_indise){
  if(dataset == "ECG200"){
    temp = rep(sax_rep[i], times = 4)
    sax_rep_all = append(sax_rep_all,temp)
  }
  if(dataset == "PowerCons"){
    temp = rep(sax_rep[i], times = 4)
    sax_rep_all = append(sax_rep_all,temp)
  }
  if(dataset == "Plane"){
    temp = rep(sax_rep[i], times = 4)
    sax_rep_all = append(sax_rep_all,temp)
  }
  if(dataset == "GunPointMaleVersusFemale"){
    
    if(control != 37){
      temp = rep(sax_rep[i], times = 4)
      sax_rep_all = append(sax_rep_all,temp)
      control = control + 1
    }
    else{
      temp = rep(sax_rep[i], times = 2)
      sax_rep_all = append(sax_rep_all,temp)
      control = 0
    }
  }
  if(dataset == "GunPointAgeSpan"){
    if(control != 37){
      temp = rep(sax_rep[i], times = 4)
      sax_rep_all = append(sax_rep_all,temp)
      control = control + 1
    }
    else{
      temp = rep(sax_rep[i], times = 2)
      sax_rep_all = append(sax_rep_all,temp)
      control = 0
    }
  }
}

long_train[,sax_rep_char := sax_rep_all]
long_train[,sax_rep_char_num := as.numeric(as.factor(sax_rep_all))]  
long_train[,sax_rep:=mean(value),by = list(id,sax_rep_char_num)]
long_train$sax_rep = as.numeric(long_train$sax_rep)
head(long_train)

```

Here, we have a new representation with no needed columns but I will delete those columns later.

#### Parameter Set 2 (segment length = 8, alphabet size = 4)

Defining needed variables:

```{r, Set parameters for SAX2, warning=F,fig.width=10}

sax_segment_length=8
sax_alphabet_size=4
sax_rep_2 = vector("character",)
sax_rep_all_2 = vector("character",)
loop_indise = n_series_train * ceiling(tlength/8)

```

```{r, SAX2, warning=F,fig.width=10}

for( i in 1:n_series_train){
  
  selected_series = i
  temp_data_ts = long_train[id == selected_series]$value
  temp_sax_rep=repr_sax(temp_data_ts, q = sax_segment_length, a = sax_alphabet_size)
  sax_rep_2 = append(sax_rep_2, temp_sax_rep)
  
}

```

```{r, SAX2 2, warning=F,fig.width=10}

for( i in 1:loop_indise){
  if(dataset == "ECG200"){
    temp = rep(sax_rep_2[i], times = 8)
    sax_rep_all_2 = append(sax_rep_all_2,temp)
  }
  if(dataset == "PowerCons"){
    temp = rep(sax_rep_2[i], times = 8)
    sax_rep_all_2 = append(sax_rep_all_2,temp)
  }
  if(dataset == "Plane"){
    temp = rep(sax_rep_2[i], times = 8)
    sax_rep_all_2 = append(sax_rep_all_2,temp)
  }
  if(dataset == "GunPointMaleVersusFemale"){
    if(control != 18){
      temp = rep(sax_rep_2[i], times = 8)
      sax_rep_all_2 = append(sax_rep_all_2,temp)
      control = control + 1
    }
    else{
      temp = rep(sax_rep_2[i], times = 6)
      sax_rep_all_2 = append(sax_rep_all_2,temp)
      control = 0
    }
  }
  if(dataset == "GunPointAgeSpan"){
    if(control != 18){
      temp = rep(sax_rep_2[i], times = 8)
      sax_rep_all_2 = append(sax_rep_all_2,temp)
      control = control + 1
    }
    else{
      temp = rep(sax_rep_2[i], times = 6)
      sax_rep_all_2 = append(sax_rep_all_2,temp)
      control = 0
    }
  }
}

long_train[,sax_rep_char_2 := sax_rep_all_2]
long_train[,sax_rep_char_num_2 := as.numeric(as.factor(sax_rep_all_2))]  
long_train[,sax_rep_2:=mean(value),by = list(id,sax_rep_char_num_2)]
head(long_train)

```

Now, we can get rid of all  those no needed columns.

```{r, SAX2 3, warning=F,fig.width=10}

long_train = long_train[,-c("sax_rep_char","sax_rep_char_num","sax_rep_char_2","sax_rep_char_num_2")]
head(long_train)

```

So, we are done with represent our training data. However, wee need to merge those representations with test data in order to make some classification.First thing, I need to melt the data to wide format and merge them with test data.


```{r, Melt the data columns, warning=F,fig.width=10}

#Melt the data columns

raw_rep_long_train <- long_train[,-c("class","paa_rep","sax_rep","sax_rep_2","paa_rep_2")]
wide_raw_rep <- reshape(raw_rep_long_train, idvar = "id", v.names = "value", timevar = "time", direction = "wide")
wide_raw_rep_with_test <- rbind(wide_raw_rep,testdata, use.names=FALSE)

paa_rep_long_train <- long_train[,-c("class","value","sax_rep","sax_rep_2","paa_rep_2")]
wide_paa_rep <- reshape(paa_rep_long_train, idvar = "id", v.names = "paa_rep", timevar = "time", direction = "wide")
wide_paa_rep_with_test <- rbind(wide_paa_rep,testdata, use.names=FALSE)


paa_rep_2_long_train <- long_train[,-c("class","value","sax_rep","sax_rep_2","paa_rep")]
wide_paa_rep_2 <- reshape(paa_rep_2_long_train, idvar = "id", v.names = "paa_rep_2", timevar = "time", direction = "wide")
wide_paa_rep_2_with_test <- rbind(wide_paa_rep_2,testdata, use.names=FALSE)


sax_rep_long_train <- long_train[,-c("class","value","paa_rep","sax_rep_2","paa_rep_2")]
wide_sax_rep <- reshape(sax_rep_long_train, idvar = "id", v.names = "sax_rep", timevar = "time", direction = "wide")
wide_sax_rep_with_test <- rbind(wide_sax_rep,testdata, use.names=FALSE)

sax_rep_2_long_train <- long_train[,-c("class","value","sax_rep","paa_rep","paa_rep_2")]
wide_sax_rep_2 <- reshape(sax_rep_2_long_train, idvar = "id", v.names = "sax_rep_2", timevar = "time", direction = "wide")
wide_sax_rep_2_with_test <- rbind(wide_sax_rep_2,testdata, use.names=FALSE)

```

In order to calculate distances, I need to get rid of the first column of the data which is class information. 

```{r,Drop first column from data, warning=F,fig.width=10}

# Drop first column from data

traindata <- traindata[,2:(ncol(traindata)-1)]
testdata <- testdata[,2:(ncol(testdata))]
wide_raw_rep_with_test <- wide_raw_rep_with_test[,2:ncol(wide_raw_rep_with_test)]
wide_paa_rep_with_test <- wide_paa_rep_with_test[,2:ncol(wide_paa_rep_with_test)]
wide_paa_rep_2_with_test <- wide_paa_rep_2_with_test[,2:ncol(wide_paa_rep_2_with_test)]
wide_sax_rep_with_test <- wide_sax_rep_with_test[,2:ncol(wide_sax_rep_with_test)]
wide_sax_rep_2_with_test <- wide_sax_rep_2_with_test[,2:ncol(wide_sax_rep_2_with_test)]

```

### Euclidean Distance

In order to calculate euclidean distance, I used `dist` function.

```{r,Euclidean Distance 1, warning=F,fig.width=10}

#Euclidean Distance
raw_dist_euc <- as.matrix(dist(wide_raw_rep_with_test))
paa_dist_euc <- as.matrix(dist(wide_paa_rep_with_test))
paa2_dist_euc <- as.matrix(dist(wide_paa_rep_2_with_test))
sax_dist_euc <- as.matrix(dist(wide_sax_rep_with_test))
sax2_dist_euc <- as.matrix(dist(wide_sax_rep_2_with_test))

```

I added a big number into the diagonal of the each matrix to have a proper NN classification.

```{r,Euclidean Distance 2, warning=F,fig.width=10}
large_number = 100000000
diag(raw_dist_euc) = large_number
diag(paa_dist_euc) = large_number
diag(paa2_dist_euc) = large_number
diag(sax_dist_euc) = large_number
diag(sax2_dist_euc) = large_number

```

Because calculating all data has time consuming, I will going to store those distance matrices in my local. 

```{r,Euclidean Distance 3, warning=F,fig.width=10}

fwrite(raw_dist_euc,sprintf('%s/%s_raw_dist_euc.csv',dist_path,dataset),col.names=F)
fwrite(paa_dist_euc,sprintf('%s/%s_paa_dist_euc.csv',dist_path,dataset),col.names=F)
fwrite(paa2_dist_euc,sprintf('%s/%s_paa2_dist_euc.csv',dist_path,dataset),col.names=F)
fwrite(sax_dist_euc,sprintf('%s/%s_sax_dist_euc.csv',dist_path,dataset),col.names=F)
fwrite(sax2_dist_euc,sprintf('%s/%s_sax2_dist_euc.csv',dist_path,dataset),col.names=F)

```

### Dynamic Time Warping (DTW)

In order to calculate DTW distance, I used `dtwDist` function. Also, I followed the same steps as Euclidean distance.

```{r,DTW Distance 1, warning=F,fig.width=10}

#DTW Distance
raw_dist_dtw=as.matrix(dtwDist(wide_raw_rep_with_test))
paa_dist_dtw=as.matrix(dtwDist(wide_paa_rep_with_test))
paa2_dist_dtw=as.matrix(dtwDist(wide_paa_rep_2_with_test))
sax_dist_dtw=as.matrix(dtwDist(wide_sax_rep_with_test))
sax2_dist_dtw=as.matrix(dtwDist(wide_sax_rep_2_with_test))

```

```{r,DTW Distance 2, warning=F,fig.width=10}

diag(raw_dist_dtw)=large_number
diag(paa_dist_dtw) = large_number
diag(paa2_dist_dtw) = large_number
diag(sax_dist_dtw) = large_number
diag(sax2_dist_dtw) = large_number

```

```{r,DTW Distance 3, warning=F,fig.width=10}

fwrite(raw_dist_dtw,sprintf('%s/%s_raw_dist_dtw.csv',dist_path,dataset),col.names=F)
fwrite(paa_dist_dtw,sprintf('%s/%s_paa_dist_dtw.csv',dist_path,dataset),col.names=F)
fwrite(paa2_dist_dtw,sprintf('%s/%s_paa2_dist_dtw.csv',dist_path,dataset),col.names=F)
fwrite(sax_dist_dtw,sprintf('%s/%s_sax_dist_dtw.csv',dist_path,dataset),col.names=F)
fwrite(sax2_dist_dtw,sprintf('%s/%s_sax2_dist_dtw.csv',dist_path,dataset),col.names=F)

```

### LCSS

In order to calculate LCSS distance, I used `TSDatabaseDistances` function with parameter epsilon = 0.05. Also, I followed the same steps as Euclidean distance.

```{r,LCSS Distance 1, warning=F,fig.width=10}

#LCSS Distance

raw_dist_lcss=TSDatabaseDistances(wide_raw_rep_with_test,distance='lcss',epsilon=0.05)
raw_dist_lcss=as.matrix(raw_dist_lcss)
diag(raw_dist_lcss)=large_number

paa_dist_lcss=TSDatabaseDistances(wide_paa_rep_with_test,distance='lcss',epsilon=0.05)
paa_dist_lcss=as.matrix(paa_dist_lcss)
diag(paa_dist_lcss)=large_number

paa2_dist_lcss=TSDatabaseDistances(wide_paa_rep_2_with_test,distance='lcss',epsilon=0.05)
paa2_dist_lcss=as.matrix(paa2_dist_lcss)
diag(paa2_dist_lcss)=large_number

sax_dist_lcss=TSDatabaseDistances(wide_sax_rep_with_test,distance='lcss',epsilon=0.05)
sax_dist_lcss=as.matrix(sax_dist_lcss)
diag(sax_dist_lcss)=large_number

sax2_dist_lcss=TSDatabaseDistances(wide_sax_rep_2_with_test,distance='lcss',epsilon=0.05)
sax2_dist_lcss=as.matrix(sax2_dist_lcss)
diag(sax2_dist_lcss)=large_number

```

```{r,LCSS Distance 2, warning=F,fig.width=10}

fwrite(raw_dist_lcss,sprintf('%s/%s_raw_dist_lcss.csv',dist_path,dataset),col.names=F)
fwrite(paa_dist_lcss,sprintf('%s/%s_paa_dist_lcss.csv',dist_path,dataset),col.names=F)
fwrite(paa2_dist_lcss,sprintf('%s/%s_paa2_dist_lcss.csv',dist_path,dataset),col.names=F)
fwrite(sax_dist_lcss,sprintf('%s/%s_sax_dist_lcss.csv',dist_path,dataset),col.names=F)
fwrite(sax2_dist_lcss,sprintf('%s/%s_sax2_dist_lcss.csv',dist_path,dataset),col.names=F)

```

### ERP Distance

In order to calculate ERP distance, I used `TSDatabaseDistances` function with parameter g = 0.05. Also, I followed the same steps as Euclidean distance.

```{r,ERP Distance 1, warning=F,fig.width=10}

#ERP Distance

raw_dist_erp=TSDatabaseDistances(wide_raw_rep_with_test,distance='erp',g=0.5)
raw_dist_erp=as.matrix(raw_dist_erp)
diag(raw_dist_erp)=large_number

paa_dist_erp=TSDatabaseDistances(wide_paa_rep_with_test,distance='erp',g=0.5)
paa_dist_erp=as.matrix(paa_dist_erp)
diag(paa_dist_erp)=large_number

paa2_dist_erp=TSDatabaseDistances(wide_paa_rep_2_with_test,distance='erp',g=0.5)
paa2_dist_erp=as.matrix(paa2_dist_erp)
diag(paa2_dist_erp)=large_number

sax_dist_erp=TSDatabaseDistances(wide_sax_rep_with_test,distance='erp',g=0.5)
sax_dist_erp=as.matrix(sax_dist_erp)
diag(sax_dist_erp)=large_number

sax2_dist_erp=TSDatabaseDistances(wide_sax_rep_2_with_test,distance='erp',g=0.5)
sax2_dist_erp=as.matrix(sax2_dist_erp)
diag(sax2_dist_erp)=large_number

```

```{r,ERP Distance 2, warning=F,fig.width=10}

fwrite(raw_dist_erp,sprintf('%s/%s_raw_dist_erp.csv',dist_path,dataset),col.names=F)
fwrite(paa_dist_erp,sprintf('%s/%s_paa_dist_erp.csv',dist_path,dataset),col.names=F)
fwrite(paa2_dist_erp,sprintf('%s/%s_paa2_dist_erp.csv',dist_path,dataset),col.names=F)
fwrite(sax_dist_erp,sprintf('%s/%s_sax_dist_erp.csv',dist_path,dataset),col.names=F)
fwrite(sax2_dist_erp,sprintf('%s/%s_sax2_dist_erp.csv',dist_path,dataset),col.names=F)

```

Now, we have the information for 5 different representation with 4 different distance measurements. There is a final step which is classification. As I mentioned before, I will going to use Neighbourhood Search with Cross Validation approach.

### Classification

As I mentioned before, I will going to use Neighbourhood Search with Cross Validation approach. So for NN Classification, there is a function called `nn_classify_cv` do all the predictions and returns a list of it.

```{r,NN Classification, warning=F,fig.width=10}

nn_classify_cv=function(dist_matrix,train_class,test_indices,k=1){
  
  test_distances_to_train=dist_matrix[test_indices,]
  test_distances_to_train=test_distances_to_train[,-test_indices]
  train_class=train_class[-test_indices]
  #print(str(test_distances_to_train))
  ordered_indices=apply(test_distances_to_train,1,order)
  if(k==1){
    nearest_class=as.numeric(allclass[as.numeric(ordered_indices[1,])])
    nearest_class=data.table(id=test_indices,nearest_class)
  } else {
    nearest_class=apply(ordered_indices[1:k,],2,function(x) {allclass[x]})
    nearest_class=data.table(id=test_indices,t(nearest_class))
  }
  
  long_nn_class=melt(nearest_class,'id')
  
  class_counts=long_nn_class[,.N,list(id,value)]
  class_counts[,predicted_prob:=N/k]
  wide_class_prob_predictions=dcast(class_counts,id~value,value.var='predicted_prob')
  wide_class_prob_predictions[is.na(wide_class_prob_predictions)]=0
  class_predictions=class_counts[,list(predicted=value[which.max(N)]),by=list(id)]
  
  
  return(list(prediction=class_predictions,prob_estimates=wide_class_prob_predictions))
  
}

```

#### Cross Validation

Now, we can use that function in the cross validation but first we need to create cross validation indices. We are going to use 10-fold cross validation with 5 repeats and k values are going to be 1,3 and 5. 

Defining needed variables:

```{r,CV variables, warning=F,fig.width=10}

set.seed(100)
nof_rep=5
n_fold=10
k_levels=c(1,3,5)
iter=1


```

Also, we need to generate Cross Validation runs with respect to those parameters.

```{r,CV indices, warning=F,fig.width=10}

cv_indices=generateCVRuns(trainclass, ntimes =nof_rep, nfold = n_fold, 
                          leaveOneOut = FALSE, stratified = TRUE)


```

So now, we can get all the distance files from our local to put them into NN classification.

```{r,getting distance folder from local, warning=F,fig.width=10}

dist_folder=sprintf('%s/ClassificationData/%s/distances/%s',current_folder,dataset,dataset)
dist_files=list.files(dist_folder, full.names=T)
approach_file=list.files(dist_folder)
result=vector('list',length(dist_files)*nof_rep*n_fold*length(k_levels))
list.files(dist_folder)

```

As we can see, we have 20 different distance files (5 representation * 4 distance measure).

So now, we are good to do cross validation with using a huge for loop below:

```{r,Cross Validation, warning=F,fig.width=10}

for(m in 1:length(dist_files)){ #
  print(dist_files[m])
  dist_mat=as.matrix(fread(dist_files[m],header=FALSE))
  for(i in 1:nof_rep){
    this_fold=cv_indices[[i]]
    for(j in 1:n_fold){
      test_indices=this_fold[[j]]
      for(k in 1:length(k_levels)){
        current_k=k_levels[k]
        current_fold=nn_classify_cv(dist_mat,allclass,test_indices,k=current_k)
        accuracy=sum(allclass[test_indices]==current_fold$prediction$predicted)/length(test_indices)
        tmp=data.table(approach=approach_file[m],repid=i,foldid=j,
                       k=current_k,acc=accuracy)
        result[[iter]]=tmp
        iter=iter+1
      }
    }
  }   
}

```

As last step, we can put all the results on a table with their mean accuracy and standard deviation accuracy. 

```{r,results, warning=F,fig.width=10}

overall_results=rbindlist(result)
summarized_results=overall_results[,list(avg_acc=mean(acc),sdev_acc=sd(acc),result_count=.N),by=list(approach,k)]
summarized_results[order(-avg_acc)]

```

I also wanted to save the result in my local:

```{r,results 2, warning=F,fig.width=10}

fwrite(summarized_results[order(-avg_acc)],sprintf('%s/%sresult.csv',main_path,dataset),col.names=T)

```

### Conclusion

With checking the results table for `ECG200`,

- We can clearly see that most powerful classification is with k = 5 cross validation. 
- In the top four, SAX representation was there.
- When we compare the parameters of the representations, as we expected with smaller segment length, it gives more accurate results. Also for SAX representation, more alphabet size gives better results most of the time. 
- For this dataset ERP distance measurement give the best results. Even tough, DTW is more complex. 
- Accuracy in mean and standard deviation are not the best but they are acceptable. Because the data classes are not very separate from each other. 

### Other Data Sets

As I mentioned before, you can run this code for different dataset with changing only `dataset` variable so you can analyze it if you have train and test data in your local.

Because all the computation in distance measurements takes a lot of time, I run the code before and get the results for each data set.

#### PowerCons

```{r,PowerCons, warning=F,fig.width=10}

PowerCons_results_path = sprintf('%s/ClassificationData/PowerCons/PowerConsresult.csv',current_folder)
PowerCons_results=as.data.table(fread(PowerCons_results_path))
print(PowerCons_results)

```

In this dataset, we also had 2 different classes and like ECG200 dataset, here also we cannot clearly separate classes easily so the results are almost the same. 

- k = 5 is the best as `ECG200`
- In this dataset, euclidean distance gaves the best results.
- But interestingly, PAA2 representation which is with segment length 10 is gave the best result for this data set. So we cannot conclude the smaller segment length is better for classification. 

#### Plane

```{r,Plane, warning=F,fig.width=10}

Plane_results_path = sprintf('%s/ClassificationData/Plane/Planeresult.csv',current_folder)
Plane_results=as.data.table(fread(Plane_results_path))
print(Plane_results)

```

In this dataset, we have 8 different classes but it does not go well. Nearest Neighbourhood search is not good for this dataset. Also, when we visualize the data with respect to its classes, we cannot see difference between the classes easily.

#### GunPointAgeSpan

```{r,GunPointAgeSpan, warning=F,fig.width=10}

GunPointAgeSpan_results_path = sprintf('%s/ClassificationData/GunPointAgeSpan/GunPointAgeSpanresult.csv',current_folder)
GunPointAgeSpan_results=as.data.table(fread(GunPointAgeSpan_results_path))
print(GunPointAgeSpan_results)

```

We have 100% success in 2 cases for this dataset. Because, classes very distinct from each other, Nearest Neighbourhood did a great job here.

- k = 3 and 5 gave the best results.
- PAA2 representation (segment length = 10) is the best one for this result.
- DTW is the best distance measurement for this dataset.

#### GunPointMaleVersusFemale

```{r,GunPointMaleVersusFemale, warning=F,fig.width=10}

GunPointMaleVersusFemale_results_path = sprintf('%s/ClassificationData/GunPointMaleVersusFemale/GunPointMaleVersusFemaleresult.csv',current_folder)
GunPointMaleVersusFemale_results=as.data.table(fread(GunPointMaleVersusFemale_results_path))
print(GunPointMaleVersusFemale_results)

```

In this data set, we have 8 different 100% success. This dataset are almost the same as GunPointAgeSpan but has small distinction from it. 

- 6 of the best results from 8 has k = 5 value.
- But only ERP and Euclidean distances could able to gave 100% result.
- Also both PAA representations with different parameters gave the best results. 













